{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialasing pyspark\n",
    "\n",
    "We will work with pyspark to better read the original txt files. We do not need a validation file, we will generate our own dataset and then divide it between train and test set at 75/25 proportion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext.getOrCreate()\n",
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating one single dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lenght of each set\n",
      "train:  3257\n",
      "test:  1421\n",
      "val:  374\n",
      "Final data:  5052\n"
     ]
    }
   ],
   "source": [
    "data_names = [\"train\", \"test\", \"val\"]\n",
    "schemaString = \"idx tweet label\"\n",
    "fields = [StructField(field_name, StringType(), True) for field_name in schemaString.split()]\n",
    "schema = StructType(fields)\n",
    "dfs = []\n",
    "n_features = 100\n",
    "ngrams = 2\n",
    "print(\"Lenght of each set\")\n",
    "for data in data_names:\n",
    "    text = sc.textFile('emotion/'+data+'_text.txt', use_unicode=True).map(lambda x: x.split('\\n')).zipWithIndex()\n",
    "    labels = sc.textFile('emotion/'+data+'_labels.txt', use_unicode=True).map(lambda x: x.split('\\n')).zipWithIndex()\n",
    "    t_idx = text.map(lambda x: (x[1], x[0][0]))\n",
    "    l_idx = labels.map(lambda x: (x[1], x[0][0]))\n",
    "    datasets = t_idx.join(l_idx).map(lambda x: (x[0], x[1][0], x[1][1]))\n",
    "    df = spark.createDataFrame(datasets, schema).drop('idx')\n",
    "    dfs.append(df)\n",
    "    print(data+': ', df.count())\n",
    "final = dfs[0].union(dfs[1]).union(dfs[2])\n",
    "print(\"Final data: \", final.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to pandas dataframe\n",
    "final_pd = final.toPandas().sample(frac=1,random_state=1492)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2118\n",
       "3    1326\n",
       "1    1163\n",
       "2     445\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To see the imbalance of the data\n",
    "final_pd.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pd.to_csv('Data/all_tweets.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = final_pd.tweet\n",
    "y = final_pd.label\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1492, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3789,), (3789,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv('Data/X_train.csv', index=False)\n",
    "y_train.to_csv('Data/y_train.csv', index=False)\n",
    "X_test.to_csv('Data/X_test.csv', index=False)\n",
    "y_test.to_csv('Data/y_test.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
